**Problem 5**

Consider an LSSM model with system matrices \( A, B, C, D \). Prove the following identities:

1. \( \displaystyle \frac{\partial \|\mathbf{y}_k - \hat{\mathbf{y}}_k\|_2^2}{\partial \hat{\mathbf{y}}_k} = 2(\hat{\mathbf{y}}_k - \mathbf{y}_k)^\top \).

2. \( \displaystyle \frac{\partial \hat{\mathbf{y}}_k}{\partial \mathbf{x}_k} = C \) and \( \displaystyle \frac{\partial \mathbf{x}_{j+1}}{\partial \mathbf{x}_j} = A \) for all \( j \geq 1 \).

---

**Solution:**

**1. Prove that**

\[
\frac{\partial \|\mathbf{y}_k - \hat{\mathbf{y}}_k\|_2^2}{\partial \hat{\mathbf{y}}_k} = 2(\hat{\mathbf{y}}_k - \mathbf{y}_k)^\top.
\]

**Proof:**

The squared Euclidean norm of the error vector is given by:

\[
\|\mathbf{y}_k - \hat{\mathbf{y}}_k\|_2^2 = (\mathbf{y}_k - \hat{\mathbf{y}}_k)^\top (\mathbf{y}_k - \hat{\mathbf{y}}_k).
\]

Let \( J = \|\mathbf{y}_k - \hat{\mathbf{y}}_k\|_2^2 \). We want to compute the gradient of \( J \) with respect to \( \hat{\mathbf{y}}_k \):

\[
\frac{\partial J}{\partial \hat{\mathbf{y}}_k} = \frac{\partial}{\partial \hat{\mathbf{y}}_k} [(\mathbf{y}_k - \hat{\mathbf{y}}_k)^\top (\mathbf{y}_k - \hat{\mathbf{y}}_k)].
\]

Expanding the expression:

\[
J = (\mathbf{y}_k - \hat{\mathbf{y}}_k)^\top (\mathbf{y}_k - \hat{\mathbf{y}}_k) = \mathbf{y}_k^\top \mathbf{y}_k - 2 \hat{\mathbf{y}}_k^\top \mathbf{y}_k + \hat{\mathbf{y}}_k^\top \hat{\mathbf{y}}_k.
\]

Taking the derivative with respect to \( \hat{\mathbf{y}}_k \):

\[
\frac{\partial J}{\partial \hat{\mathbf{y}}_k} = -2 \mathbf{y}_k^\top + 2 \hat{\mathbf{y}}_k^\top = 2 (\hat{\mathbf{y}}_k - \mathbf{y}_k)^\top.
\]

**Therefore,**

\[
\frac{\partial \|\mathbf{y}_k - \hat{\mathbf{y}}_k\|_2^2}{\partial \hat{\mathbf{y}}_k} = 2 (\hat{\mathbf{y}}_k - \mathbf{y}_k)^\top.
\]

---

**2. Prove that**

\[
\frac{\partial \hat{\mathbf{y}}_k}{\partial \mathbf{x}_k} = C \quad \text{and} \quad \frac{\partial \mathbf{x}_{j+1}}{\partial \mathbf{x}_j} = A \quad \text{for all } j \geq 1.
\]

**Proof:**

Given the state-space equations:

\[
\begin{align*}
\mathbf{x}_{k+1} &= A \mathbf{x}_k + B \mathbf{u}_k, \\
\hat{\mathbf{y}}_k &= C \mathbf{x}_k + D \mathbf{u}_k.
\end{align*}
\]

**First identity:**

Since \( \hat{\mathbf{y}}_k = C \mathbf{x}_k + D \mathbf{u}_k \), and \( \mathbf{u}_k \) is independent of \( \mathbf{x}_k \):

\[
\frac{\partial \hat{\mathbf{y}}_k}{\partial \mathbf{x}_k} = C.
\]

**Second identity:**

From \( \mathbf{x}_{j+1} = A \mathbf{x}_j + B \mathbf{u}_j \), and \( \mathbf{u}_j \) is independent of \( \mathbf{x}_j \):

\[
\frac{\partial \mathbf{x}_{j+1}}{\partial \mathbf{x}_j} = A.
\]

**Thus, both identities are proven.**

---

**Problem 6**

Prove that

\[
\frac{\partial (U X V)}{\partial X} = U V^\top.
\]

---

**Solution:**

Assume \( U \) and \( V \) are column vectors of size \( n \times 1 \), and \( X \) is an \( n \times n \) matrix. Then, the scalar function is:

\[
f(X) = U^\top X V.
\]

Our goal is to compute the gradient of \( f \) with respect to \( X \):

\[
\frac{\partial f}{\partial X}.
\]

**Computing the Gradient:**

Express \( f(X) \) in terms of the elements:

\[
f(X) = \sum_{i=1}^n \sum_{j=1}^n U_i X_{ij} V_j.
\]

The partial derivative of \( f \) with respect to \( X_{kl} \) is:

\[
\frac{\partial f}{\partial X_{kl}} = U_k V_l.
\]

Therefore, the gradient \( \frac{\partial f}{\partial X} \) is an \( n \times n \) matrix with elements \( (U V^\top)_{kl} = U_k V_l \). Hence:

\[
\frac{\partial f}{\partial X} = U V^\top.
\]

**Therefore,**

\[
\frac{\partial (U X V)}{\partial X} = U V^\top.
\]

---

**Problem**
Tensor Calculus: 
Prove that

\[
\frac{\partial \mathbf{x}_{k+1}}{\partial A} = \mathbb{I}^{(2)} \otimes \mathbf{x}_k + A : \frac{\partial \mathbf{x}_k}{\partial A},
\]

where “\(:\)” denotes the contraction product using a single index.
**Definitions:**

- \( \mathbf{x}_{k+1} = A \mathbf{x}_k + B \mathbf{u}_k \).
- The partial derivative \( \frac{\partial \mathbf{x}_{k+1}}{\partial A} \) is a tensor.

**Note:** This problem involves advanced tensor calculus and the use of Kronecker products and tensor contractions. You should provide a proof using matrix calculus and recursion.
---

**Solution:**

**Note:** This problem involves advanced tensor calculus and the use of Kronecker products and tensor contractions. We'll provide a proof using matrix calculus and recursion.

**Definitions:**

- \( \mathbf{x}_{k+1} = A \mathbf{x}_k + B \mathbf{u}_k \).
- The partial derivative \( \frac{\partial \mathbf{x}_{k+1}}{\partial A} \) is a tensor.

**Proof:**

We will proceed by induction.

**Base Case (\( k = 0 \)):**

At \( k = 0 \), assuming \( \mathbf{x}_0 \) does not depend on \( A \):

\[
\frac{\partial \mathbf{x}_{1}}{\partial A} = \frac{\partial (A \mathbf{x}_0 + B \mathbf{u}_0)}{\partial A} = \mathbf{x}_0.
\]

Since \( \mathbf{x}_0 \) is independent of \( A \), the derivative is:

\[
\frac{\partial \mathbf{x}_{1}}{\partial A} = \mathbb{I}^{(2)} \otimes \mathbf{x}_0.
\]

**Inductive Step:**

Assume that for some \( k \geq 0 \):

\[
\frac{\partial \mathbf{x}_{k}}{\partial A} = \sum_{n=0}^{k-1} A^{n} \otimes \mathbf{x}_{k-n-1}.
\]

**Compute \( \frac{\partial \mathbf{x}_{k+1}}{\partial A} \):**

Using the state equation:

\[
\mathbf{x}_{k+1} = A \mathbf{x}_k + B \mathbf{u}_k.
\]

Taking the derivative with respect to \( A \):

\[
\frac{\partial \mathbf{x}_{k+1}}{\partial A} = \mathbf{x}_k + A \frac{\partial \mathbf{x}_k}{\partial A}.
\]

**Explanation:**

- The term \( \mathbf{x}_k \) comes from differentiating \( A \mathbf{x}_k \) with respect to \( A \), treating \( \mathbf{x}_k \) as constant in that differentiation step.
- The term \( A \frac{\partial \mathbf{x}_k}{\partial A} \) comes from the chain rule, since \( \mathbf{x}_k \) depends on \( A \).

**Rewriting the Equation:**

\[
\frac{\partial \mathbf{x}_{k+1}}{\partial A} = \mathbb{I}^{(2)} \otimes \mathbf{x}_k + A \cdot \frac{\partial \mathbf{x}_k}{\partial A}.
\]

**Therefore, the identity is proven:**

\[
\frac{\partial \mathbf{x}_{k+1}}{\partial A} = \mathbb{I}^{(2)} \otimes \mathbf{x}_k + A : \frac{\partial \mathbf{x}_k}{\partial A}.
\]

---

**Problem 8**

Explain in your own words the vanishing and exploding gradient problem. What techniques have we covered in this chapter to alleviate this issue?

---

**Solution:**

**Vanishing and Exploding Gradient Problem:**

When training deep neural networks or systems that involve sequential multiplications (like recurrent neural networks), gradients computed during backpropagation can either diminish (vanish) or grow exponentially (explode) as they are propagated back through layers or time steps.

**Vanishing Gradient:**

- **Definition:** The gradient becomes increasingly small as it is propagated backward, approaching zero.
- **Cause:** This occurs when the derivative of the activation function is less than one (e.g., sigmoid, tanh). Repeated multiplication of small numbers leads to an exponentially decreasing gradient.
- **Effect:** Early layers (closer to the input) learn very slowly or not at all because their weights are updated negligibly.

**Exploding Gradient:**

- **Definition:** The gradient increases exponentially as it is propagated backward, becoming very large.
- **Cause:** When the derivative of the activation function or the weights is greater than one. Repeated multiplication leads to exponentially increasing gradients.
- **Effect:** Causes numerical instability, making the learning process erratic. The weights can become too large, leading to model divergence.

**Techniques to Alleviate the Issue:**

1. **Weight Initialization:**

   - **Glorot/Xavier Initialization:** Sets initial weights to keep the variance of activations constant across layers.
   - **He Initialization:** Adjusted for ReLU activations, further mitigating gradient issues.

2. **Activation Functions:**

   - **ReLU Activation:** Rectified Linear Unit avoids saturation in positive range, helping prevent vanishing gradients.
   - **Leaky ReLU and ELU:** Provide small gradients for negative inputs, aiding gradient flow.

3. **Normalization Techniques:**

   - **Batch Normalization:** Normalizes inputs of each layer to maintain consistent gradient flow.

4. **Gradient Clipping:**

   - **Clipping Gradients:** Imposes a threshold on gradients to prevent them from exploding beyond a certain value.

5. **Residual Connections:**

   - **Skip Connections (ResNets):** Allow gradients to bypass certain layers, reducing the vanishing gradient effect.

6. **Using Gated Units:**

   - **LSTM and GRU:** In recurrent networks, these units manage the flow of information, addressing vanishing gradients over time steps.

7. **Regularization Techniques:**

   - **Adding Penalties:** Techniques like L2 regularization prevent weights from growing too large.

**In This Chapter:**

- **Normalization Methods:** We covered batch normalization, which helps in stabilizing the gradient flow.
- **Activation Functions:** The use of ReLU and its variants to maintain non-vanishing gradients.
- **Proper Initialization:** Emphasized initializing weights to appropriate scales to prevent gradients from vanishing or exploding.
- **Gradient Clipping:** Discussed clipping gradients during backpropagation to control their magnitude.

---

**Summary:**

Understanding the vanishing and exploding gradient problem is crucial for designing and training deep learning models effectively. By implementing the techniques mentioned, we can ensure that gradients remain at manageable levels, allowing the model to learn efficiently across all layers.